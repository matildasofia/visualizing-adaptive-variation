"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.BBI = void 0;
const buffer_1 = require("buffer");
const binary_parser_1 = require("binary-parser");
const generic_filehandle_1 = require("generic-filehandle");
const rxjs_1 = require("rxjs");
const operators_1 = require("rxjs/operators");
const blockView_1 = require("./blockView");
const BIG_WIG_MAGIC = -2003829722;
const BIG_BED_MAGIC = -2021002517;
function toString(arr) {
    return new TextDecoder().decode(arr);
}
/* get the compiled parsers for different sections of the bigwig file
 *
 * @param isBE - is big endian, typically false
 * @return an object with compiled parsers
 */
function getParsers(isBE) {
    const le = isBE ? 'big' : 'little';
    const headerParser = new binary_parser_1.Parser()
        .endianess(le)
        .int32('magic')
        .uint16('version')
        .uint16('numZoomLevels')
        .uint64('chromTreeOffset')
        .uint64('unzoomedDataOffset')
        .uint64('unzoomedIndexOffset')
        .uint16('fieldCount')
        .uint16('definedFieldCount')
        .uint64('asOffset') // autoSql offset, used in bigbed
        .uint64('totalSummaryOffset')
        .uint32('uncompressBufSize')
        .uint64('extHeaderOffset') // name index offset, used in bigbed
        .array('zoomLevels', {
        length: 'numZoomLevels',
        type: new binary_parser_1.Parser()
            .endianess(le)
            .uint32('reductionLevel')
            .uint32('reserved')
            .uint64('dataOffset')
            .uint64('indexOffset'),
    });
    const totalSummaryParser = new binary_parser_1.Parser()
        .endianess(le)
        .uint64('basesCovered')
        .doublele('scoreMin')
        .doublele('scoreMax')
        .doublele('scoreSum')
        .doublele('scoreSumSquares');
    const chromTreeParser = new binary_parser_1.Parser()
        .endianess(le)
        .uint32('magic')
        .uint32('blockSize')
        .uint32('keySize')
        .uint32('valSize')
        .uint64('itemCount');
    const isLeafNode = new binary_parser_1.Parser()
        .endianess(le)
        .uint8('isLeafNode')
        .skip(1)
        .uint16('cnt')
        .saveOffset('offset');
    return {
        chromTreeParser,
        totalSummaryParser,
        headerParser,
        isLeafNode,
    };
}
class BBI {
    /* fetch and parse header information from a bigwig or bigbed file
     * @param abortSignal - abort the operation, can be null
     * @return a Header object
     */
    getHeader(opts = {}) {
        const options = 'aborted' in opts ? { signal: opts } : opts;
        if (!this.headerP) {
            this.headerP = this._getHeader(options).catch(e => {
                this.headerP = undefined;
                throw e;
            });
        }
        return this.headerP;
    }
    /*
     * @param filehandle - a filehandle from generic-filehandle or implementing something similar to the node10 fs.promises API
     * @param path - a Local file path as a string
     * @param url - a URL string
     * @param renameRefSeqs - an optional method to rename the internal reference sequences using a mapping function
     */
    constructor(options = {}) {
        const { filehandle, renameRefSeqs = s => s, path, url } = options;
        this.renameRefSeqs = renameRefSeqs;
        if (filehandle) {
            this.bbi = filehandle;
        }
        else if (url) {
            this.bbi = new generic_filehandle_1.RemoteFile(url);
        }
        else if (path) {
            this.bbi = new generic_filehandle_1.LocalFile(path);
        }
        else {
            throw new Error('no file given');
        }
    }
    _getHeader(opts) {
        return __awaiter(this, void 0, void 0, function* () {
            const header = yield this._getMainHeader(opts);
            const chroms = yield this._readChromTree(header, opts);
            return Object.assign(Object.assign({}, header), chroms);
        });
    }
    _getMainHeader(opts, requestSize = 2000) {
        return __awaiter(this, void 0, void 0, function* () {
            const { buffer } = yield this.bbi.read(buffer_1.Buffer.alloc(requestSize), 0, requestSize, 0, opts);
            const isBigEndian = this._isBigEndian(buffer);
            const ret = getParsers(isBigEndian);
            const header = ret.headerParser.parse(buffer);
            const { magic, asOffset, totalSummaryOffset } = header;
            header.fileType = magic === BIG_BED_MAGIC ? 'bigbed' : 'bigwig';
            if (asOffset > requestSize || totalSummaryOffset > requestSize) {
                return this._getMainHeader(opts, requestSize * 2);
            }
            if (asOffset) {
                const off = Number(header.asOffset);
                header.autoSql = toString(buffer.subarray(off, buffer.indexOf(0, off)));
            }
            if (header.totalSummaryOffset > requestSize) {
                return this._getMainHeader(opts, requestSize * 2);
            }
            if (header.totalSummaryOffset) {
                const tail = buffer.subarray(Number(header.totalSummaryOffset));
                const sum = ret.totalSummaryParser.parse(tail);
                header.totalSummary = Object.assign(Object.assign({}, sum), { basesCovered: Number(sum.basesCovered) });
            }
            return Object.assign(Object.assign({}, header), { isBigEndian });
        });
    }
    _isBigEndian(buffer) {
        let ret = buffer.readInt32LE(0);
        if (ret === BIG_WIG_MAGIC || ret === BIG_BED_MAGIC) {
            return false;
        }
        ret = buffer.readInt32BE(0);
        if (ret === BIG_WIG_MAGIC || ret === BIG_BED_MAGIC) {
            return true;
        }
        throw new Error('not a BigWig/BigBed file');
    }
    // todo: add progress if long running
    _readChromTree(header, opts) {
        return __awaiter(this, void 0, void 0, function* () {
            const isBE = header.isBigEndian;
            const le = isBE ? 'big' : 'little';
            const refsByNumber = [];
            const refsByName = {};
            let unzoomedDataOffset = Number(header.unzoomedDataOffset);
            const chromTreeOffset = Number(header.chromTreeOffset);
            while (unzoomedDataOffset % 4 !== 0) {
                unzoomedDataOffset += 1;
            }
            const off = unzoomedDataOffset - chromTreeOffset;
            const { buffer } = yield this.bbi.read(buffer_1.Buffer.alloc(off), 0, off, Number(chromTreeOffset), opts);
            const p = getParsers(isBE);
            const { keySize } = p.chromTreeParser.parse(buffer);
            const leafNodeParser = new binary_parser_1.Parser()
                .endianess(le)
                .string('key', { stripNull: true, length: keySize })
                .uint32('refId')
                .uint32('refSize')
                .saveOffset('offset');
            const nonleafNodeParser = new binary_parser_1.Parser()
                .endianess(le)
                .skip(keySize)
                .uint64('childOffset')
                .saveOffset('offset');
            const rootNodeOffset = 32;
            const bptReadNode = (currentOffset) => __awaiter(this, void 0, void 0, function* () {
                let offset = currentOffset;
                if (offset >= buffer.length) {
                    throw new Error('reading beyond end of buffer');
                }
                const ret = p.isLeafNode.parse(buffer.subarray(offset));
                const { isLeafNode, cnt } = ret;
                offset += ret.offset;
                if (isLeafNode) {
                    for (let n = 0; n < cnt; n += 1) {
                        const leafRet = leafNodeParser.parse(buffer.subarray(offset));
                        offset += leafRet.offset;
                        const { key, refId, refSize } = leafRet;
                        const refRec = { name: key, id: refId, length: refSize };
                        refsByName[this.renameRefSeqs(key)] = refId;
                        refsByNumber[refId] = refRec;
                    }
                }
                else {
                    // parse index node
                    const nextNodes = [];
                    for (let n = 0; n < cnt; n += 1) {
                        const nonleafRet = nonleafNodeParser.parse(buffer.subarray(offset));
                        const { childOffset } = nonleafRet;
                        offset += nonleafRet.offset;
                        nextNodes.push(bptReadNode(Number(childOffset) - Number(chromTreeOffset)));
                    }
                    yield Promise.all(nextNodes);
                }
            });
            yield bptReadNode(rootNodeOffset);
            return {
                refsByName,
                refsByNumber,
            };
        });
    }
    /*
     * fetches the "unzoomed" view of the bigwig data. this is the default for bigbed
     * @param abortSignal - a signal to optionally abort this operation
     */
    getUnzoomedView(opts) {
        return __awaiter(this, void 0, void 0, function* () {
            const { unzoomedIndexOffset, refsByName, uncompressBufSize, isBigEndian, fileType, } = yield this.getHeader(opts);
            return new blockView_1.BlockView(this.bbi, refsByName, unzoomedIndexOffset, isBigEndian, uncompressBufSize > 0, fileType);
        });
    }
    /**
     * Gets features from a BigWig file
     *
     * @param refName - The chromosome name
     * @param start - The start of a region
     * @param end - The end of a region
     * @param opts - An object containing basesPerSpan (e.g. pixels per basepair) or scale used to infer the zoomLevel to use
     */
    getFeatureStream(refName, start, end, opts = {
        scale: 1,
    }) {
        return __awaiter(this, void 0, void 0, function* () {
            yield this.getHeader(opts);
            const chrName = this.renameRefSeqs(refName);
            let view;
            if (opts.basesPerSpan) {
                view = yield this.getView(1 / opts.basesPerSpan, opts);
            }
            else if (opts.scale) {
                view = yield this.getView(opts.scale, opts);
            }
            else {
                view = yield this.getView(1, opts);
            }
            if (!view) {
                throw new Error('unable to get block view for data');
            }
            return new rxjs_1.Observable((observer) => {
                view.readWigData(chrName, start, end, observer, opts);
            });
        });
    }
    getFeatures(refName, start, end, opts = {
        scale: 1,
    }) {
        return __awaiter(this, void 0, void 0, function* () {
            const ob = yield this.getFeatureStream(refName, start, end, opts);
            const ret = yield (0, rxjs_1.firstValueFrom)(ob.pipe((0, operators_1.toArray)()));
            return ret.flat();
        });
    }
}
exports.BBI = BBI;
//# sourceMappingURL=bbi.js.map